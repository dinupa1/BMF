{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee288064-dc4d-443a-b6db-81db93af548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import uproot\n",
    "import awkward as ak\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99abfd69-c9d6-4656-8a50-2f6266bcc781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate weights analytically\n",
    "\n",
    "def weight_fn(xx1, xx2, xx3, phi, costh):\n",
    "    weight = 1. + xx1 * costh * costh + 2. * xx2 * costh * np.sqrt(1. - costh * costh) * np.cos(phi) + 0.5 * xx3 * (1. - costh * costh)* np.cos(2. * phi)\n",
    "    return weight / (1. + costh * costh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513f72bb-4934-4ed5-8f7a-6b089ebc1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6949271-c398-4503-8d61-e9debf040050",
   "metadata": {},
   "source": [
    "# Extracting Drell-Yan Angular Coefficients using Neural Network-based Classifiers with E906 LH2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e31528-50d7-40db-8495-ef161882ff12",
   "metadata": {},
   "source": [
    "In this notebook, we use deep neural network-based classifiers to extract the DY angular coefficients. As the first step, let's define our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b721ba00-05e5-4e10-8633-33aae1a79814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMFClassifier(nn.Module):\n",
    "    def __init__(self, input_dim: int = 8, output_dim: int = 1, hidden_dim: int = 32):\n",
    "        super(BMFClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "        self.bn4 = nn.BatchNorm1d(output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.fc1(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(self.fc2(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.bn3(self.fc3(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.bn4(self.fc4(x))\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ca7162-56f8-419b-8fe3-2e0a80522147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module used to add parameter for fitting\n",
    "class AddParams2Input(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(AddParams2Input, self).__init__()\n",
    "        self.params = nn.Parameter(torch.Tensor(params), requires_grad=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_params = torch.ones((inputs.size(0), 1), device=inputs.device) * self.params.to(device=inputs.device)\n",
    "        concatenated = torch.cat([inputs, batch_params], dim=-1)\n",
    "        return concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45701840-7ccd-46a9-a01a-af148d28c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "class BMFLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BMFLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, outputs, targets, weights):\n",
    "        criterion = nn.BCELoss(reduction=\"none\")\n",
    "        loss = criterion(outputs, targets)\n",
    "        weighted_loss = loss* weights\n",
    "        return weighted_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99353960-d6bc-4fb0-b036-f4049b77c0bd",
   "metadata": {},
   "source": [
    "We extract the angular coefficients in two steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405bd9e-e17b-4f4e-b1bd-f0ec55ad924d",
   "metadata": {},
   "source": [
    "## Step 1: Parameterize the neural network with angular coefficients\n",
    "\n",
    "In this step, we parameterize the neural network with $\\lambda$, $\\mu$, $\\nu$ values. This is done during the training step. The input features to the neural network are `mass`, `pT`, `xF`, `phi`, `costh`, `lambda`, `mu`, and `nu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78bfc182-5920-44d6-898b-41f80d3ff6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train step\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, device, epochs, early_stopping_patience):\n",
    "    best_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_inputs, batch_labels, batch_weights in train_loader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            batch_weights = batch_weights.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_labels, batch_weights)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * batch_inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            for batch_inputs, batch_labels, batch_weights in test_loader:\n",
    "                batch_inputs = batch_inputs.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                batch_weights = batch_weights.to(device)\n",
    "\n",
    "                outputs = model(batch_inputs)\n",
    "                loss = criterion(outputs, batch_labels, batch_weights)\n",
    "\n",
    "                running_loss += loss.item() * batch_inputs.size(0)\n",
    "\n",
    "            validation_loss = running_loss / len(test_loader.dataset)\n",
    "\n",
    "            print(\"Epoch {}: Train Loss = {:.4f}, Test Loss = {:.4f}\".format(epoch + 1, epoch_loss, validation_loss))\n",
    "\n",
    "            # Check for early stopping\n",
    "            if validation_loss < best_loss:\n",
    "                best_loss = validation_loss\n",
    "                best_model_weights = model.state_dict()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping at epoch {}\".format(epoch))\n",
    "                break\n",
    "\n",
    "    return best_model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71722f21-d052-4a4a-82f4-bb232caf083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight_fn(model, X_val):\n",
    "    # Move the model to CPU for evaluation\n",
    "    model = model.to(torch.device(\"cpu\"))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.Tensor(X_val)).detach().numpy().ravel()\n",
    "        weights = preds / (1.0 - preds)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08beccc8-a0be-4c7b-9276-e0581c0403c0",
   "metadata": {},
   "source": [
    "Let's train the neural network with messy MC data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38767a24-f404-41a5-929d-cef23b7a67b5",
   "metadata": {},
   "source": [
    "## Step 2: Extract the parameters using the gradient descent algorithm\n",
    "\n",
    "Since we have parameterized the neural network in step 1, we can fix the trained weights in the neural network and extract the angular coefficients by minimizing the loss with the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859d1bc9-2c2c-4998-aac3-378cfbf822ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "def fit_fn(epochs, add_params_layer, fit_model, data_loader, device, optimizer, loss_fn):\n",
    "    losses = []\n",
    "    fit_vals = {\n",
    "        \"lambda\": [],\n",
    "        \"mu\": [],\n",
    "        \"nu\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        add_params_layer.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_inputs, batch_labels, batch_weights in data_loader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            batch_weights = batch_weights.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            param_input = add_params_layer(batch_inputs)\n",
    "            output = fit_model(param_input)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(output, batch_labels, batch_weights)\n",
    "\n",
    "            # Backward pass and update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader.dataset)\n",
    "        print(\"epoch : {}, loss = {:.4f}, lambda = {:.4f}, mu = {:.4f}, nu = {:.4f}\".format(epoch + 1, epoch_loss,\n",
    "                                                                                            add_params_layer.params[0].item(),\n",
    "                                                                                            add_params_layer.params[1].item(),\n",
    "                                                                                            add_params_layer.params[2].item()))\n",
    "        losses.append(epoch_loss)\n",
    "        fit_vals[\"lambda\"].append(add_params_layer.params[0].item())\n",
    "        fit_vals[\"mu\"].append(add_params_layer.params[1].item())\n",
    "        fit_vals[\"nu\"].append(add_params_layer.params[2].item())\n",
    "\n",
    "    return losses, fit_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9876c80-4c1c-48c9-aced-5d5382924154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# Define early stopping patience\n",
    "early_stopping_patience = 20\n",
    "\n",
    "# Define number of iterations\n",
    "iterations = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d70f1062-0cd3-4ab6-acc0-1f912c4b3b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Iteration 1 ***\n",
      "using device : cpu\n",
      "BMFClassifier(\n",
      "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (bn4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "total trainable params in fit model: 2531\n",
      "Epoch 1: Train Loss = 0.7223, Test Loss = 0.6953\n",
      "Epoch 2: Train Loss = 0.6927, Test Loss = 0.6925\n",
      "Epoch 3: Train Loss = 0.6924, Test Loss = 0.6925\n",
      "Epoch 4: Train Loss = 0.6923, Test Loss = 0.6924\n",
      "Epoch 5: Train Loss = 0.6923, Test Loss = 0.6924\n",
      "using device : cpu\n",
      "BMFClassifier(\n",
      "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (bn4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "total trainable params in fit model: 2531\n",
      "AddParams2Input()\n",
      "total trainable params in fit model: 3\n",
      "epoch : 1, loss = 0.6516, lambda = 0.6736, mu = 0.4734, nu = 0.0563\n",
      "epoch : 2, loss = 0.6515, lambda = 0.6654, mu = 0.4589, nu = 0.0762\n",
      "epoch : 3, loss = 0.6514, lambda = 0.6567, mu = 0.4358, nu = 0.0963\n",
      "epoch : 4, loss = 0.6513, lambda = 0.6434, mu = 0.4133, nu = 0.1119\n",
      "epoch : 5, loss = 0.6512, lambda = 0.6322, mu = 0.3973, nu = 0.1338\n",
      "*** Iteration 2 ***\n",
      "using device : cpu\n",
      "BMFClassifier(\n",
      "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (bn4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "total trainable params in fit model: 2531\n",
      "Epoch 1: Train Loss = 0.7225, Test Loss = 0.6964\n",
      "Epoch 2: Train Loss = 0.6928, Test Loss = 0.6925\n",
      "Epoch 3: Train Loss = 0.6923, Test Loss = 0.6924\n",
      "Epoch 4: Train Loss = 0.6923, Test Loss = 0.6923\n",
      "Epoch 5: Train Loss = 0.6922, Test Loss = 0.6922\n",
      "using device : cpu\n",
      "BMFClassifier(\n",
      "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (bn4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "total trainable params in fit model: 2531\n",
      "AddParams2Input()\n",
      "total trainable params in fit model: 3\n",
      "epoch : 1, loss = 0.6524, lambda = 1.2100, mu = -0.0597, nu = -0.2855\n",
      "epoch : 2, loss = 0.6522, lambda = 1.1937, mu = -0.0737, nu = -0.2639\n",
      "epoch : 3, loss = 0.6521, lambda = 1.1765, mu = -0.0826, nu = -0.2430\n",
      "epoch : 4, loss = 0.6520, lambda = 1.1630, mu = -0.0932, nu = -0.2188\n",
      "epoch : 5, loss = 0.6519, lambda = 1.1493, mu = -0.1006, nu = -0.2017\n",
      "*** Iteration 3 ***\n",
      "using device : cpu\n",
      "BMFClassifier(\n",
      "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (bn4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "total trainable params in fit model: 2531\n",
      "Epoch 1: Train Loss = 0.7222, Test Loss = 0.6962\n",
      "Epoch 2: Train Loss = 0.6930, Test Loss = 0.6924\n",
      "Epoch 3: Train Loss = 0.6923, Test Loss = 0.6923\n",
      "Epoch 4: Train Loss = 0.6922, Test Loss = 0.6922\n",
      "Epoch 5: Train Loss = 0.6922, Test Loss = 0.6923\n",
      "using device : cpu\n",
      "BMFClassifier(\n",
      "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (bn4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "total trainable params in fit model: 2531\n",
      "AddParams2Input()\n",
      "total trainable params in fit model: 3\n",
      "epoch : 1, loss = 0.6513, lambda = 1.1401, mu = -0.2217, nu = 0.1420\n",
      "epoch : 2, loss = 0.6512, lambda = 1.1197, mu = -0.2032, nu = 0.1312\n",
      "epoch : 3, loss = 0.6511, lambda = 1.1011, mu = -0.1796, nu = 0.1247\n",
      "epoch : 4, loss = 0.6510, lambda = 1.0881, mu = -0.1600, nu = 0.1170\n",
      "epoch : 5, loss = 0.6510, lambda = 1.0739, mu = -0.1502, nu = 0.1132\n",
      "*** Iteration 4 ***\n",
      "using device : cpu\n",
      "BMFClassifier(\n",
      "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (bn4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "total trainable params in fit model: 2531\n",
      "Epoch 1: Train Loss = 0.7213, Test Loss = 0.6944\n",
      "Epoch 2: Train Loss = 0.6925, Test Loss = 0.6922\n",
      "Epoch 3: Train Loss = 0.6922, Test Loss = 0.6922\n",
      "Epoch 4: Train Loss = 0.6922, Test Loss = 0.6921\n",
      "Epoch 5: Train Loss = 0.6921, Test Loss = 0.6922\n",
      "using device : cpu\n",
      "BMFClassifier(\n",
      "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (bn4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "total trainable params in fit model: 2531\n",
      "AddParams2Input()\n",
      "total trainable params in fit model: 3\n",
      "epoch : 1, loss = 0.6528, lambda = 1.1748, mu = 0.2670, nu = -0.2495\n",
      "epoch : 2, loss = 0.6527, lambda = 1.1817, mu = 0.2573, nu = -0.2279\n",
      "epoch : 3, loss = 0.6526, lambda = 1.1863, mu = 0.2491, nu = -0.2070\n",
      "epoch : 4, loss = 0.6526, lambda = 1.1918, mu = 0.2378, nu = -0.1873\n",
      "epoch : 5, loss = 0.6525, lambda = 1.1931, mu = 0.2309, nu = -0.1710\n",
      "*** Iteration 5 ***\n",
      "using device : cpu\n",
      "BMFClassifier(\n",
      "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (bn4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "total trainable params in fit model: 2531\n",
      "Epoch 1: Train Loss = 0.7202, Test Loss = 0.6968\n",
      "Epoch 2: Train Loss = 0.6928, Test Loss = 0.6923\n",
      "Epoch 3: Train Loss = 0.6924, Test Loss = 0.6923\n",
      "Epoch 4: Train Loss = 0.6923, Test Loss = 0.6924\n",
      "Epoch 5: Train Loss = 0.6923, Test Loss = 0.6922\n",
      "using device : cpu\n",
      "BMFClassifier(\n",
      "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (bn4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "total trainable params in fit model: 2531\n",
      "AddParams2Input()\n",
      "total trainable params in fit model: 3\n",
      "epoch : 1, loss = 0.6507, lambda = 0.5680, mu = -0.2681, nu = -0.1050\n",
      "epoch : 2, loss = 0.6506, lambda = 0.5496, mu = -0.2567, nu = -0.0858\n",
      "epoch : 3, loss = 0.6505, lambda = 0.5315, mu = -0.2451, nu = -0.0669\n",
      "epoch : 4, loss = 0.6504, lambda = 0.5142, mu = -0.2351, nu = -0.0460\n",
      "epoch : 5, loss = 0.6503, lambda = 0.4930, mu = -0.2229, nu = -0.0333\n"
     ]
    }
   ],
   "source": [
    "lambda_fit, mu_fit, nu_fit = [], [], []\n",
    "\n",
    "for i in range(iterations):\n",
    "    print(\"*** Iteration {} ***\".format(i+1))\n",
    "\n",
    "    # Load E906 messy MC data\n",
    "    \n",
    "    batch_size = 1024\n",
    "    n_MC_events = 10**6\n",
    "    \n",
    "    lambda0, mu0, nu0 = 1.0, 0.0, 0.0\n",
    "\n",
    "    # Sample lambda, mu, nu values in the range (0.5, 1.5), (-0.5, 0.5), (-0.5, 0.5)\n",
    "    lambda_vals = np.random.uniform(0.5, 1.5, n_MC_events)\n",
    "    mu_vals = np.random.uniform(-0.5, 0.5, n_MC_events)\n",
    "    nu_vals = np.random.uniform(-0.5, 0.5, n_MC_events)\n",
    "    \n",
    "    tree = uproot.open(\"BMFData.root:save\")\n",
    "    events = tree.arrays([\"mass\", \"pT\", \"xF\", \"phi\", \"costh\", \"true_phi\", \"true_costh\"]).to_numpy()[:2*n_MC_events]\n",
    "    \n",
    "    data_array = np.array([list(record) for record in events])\n",
    "    \n",
    "    events0, events1 = train_test_split(data_array, test_size=0.5, shuffle=True)\n",
    "    \n",
    "    X0 = [(phi, costh, lambda1, mu1, nu1) for phi, costh, lambda1, mu1, nu1 in zip(events0[:, 3], events0[:, 4], lambda_vals, mu_vals, nu_vals)]\n",
    "    X1 = [(phi, costh, lambda1, mu1, nu1) for phi, costh, lambda1, mu1, nu1 in zip(events1[:, 3], events1[:, 4], lambda_vals, mu_vals, nu_vals)]\n",
    "    \n",
    "    weight0 = [weight_fn(lambda0, mu0, nu0, phi, costh) for phi, costh in zip(events0[:, 5], events0[:, 6])]\n",
    "    weight1 = [weight_fn(lambda1, mu1, nu1, phi, costh) for lambda1, mu1, nu1, phi, costh in zip(lambda_vals, mu_vals, nu_vals, events1[:, 5], events1[:, 6])]\n",
    "    \n",
    "    Y0 = np.zeros(n_MC_events)\n",
    "    Y1 = np.ones(n_MC_events)\n",
    "    \n",
    "    X = np.concatenate((X0, X1))\n",
    "    Y = np.concatenate((Y0, Y1)).reshape(-1, 1)\n",
    "    weight = np.concatenate((weight0, weight1)).reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "    # Convert to pytorch tensor\n",
    "    X_tensor = torch.from_numpy(X).float()\n",
    "    Y_tensor = torch.from_numpy(Y).float()\n",
    "    W_tensor = torch.from_numpy(weight).float()\n",
    "    \n",
    "    # Train test split\n",
    "    X_train, X_test, Y_train, Y_test, W_train, W_test = train_test_split(X_tensor, Y_tensor, W_tensor, test_size=0.4, shuffle=True)\n",
    "    \n",
    "    # Create dataset and data loader\n",
    "    train_dataset = TensorDataset(X_train, Y_train, W_train)\n",
    "    test_dataset = TensorDataset(X_test, Y_test, W_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    fit_model = BMFClassifier(input_dim=5, hidden_dim=32)\n",
    "    \n",
    "    # Model summary\n",
    "    print(\"using device : {}\".format(device))\n",
    "    fit_trainable_params = sum(p.numel() for p in fit_model.parameters() if p.requires_grad)\n",
    "    print(fit_model)\n",
    "    print(\"total trainable params in fit model: {}\".format(fit_trainable_params))\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = BMFLoss()\n",
    "    optimizer = optim.Adam(fit_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Move the model to GPU if available\n",
    "    fit_model = fit_model.to(device=device)\n",
    "\n",
    "    # Compile the train function\n",
    "    opt_train = torch.compile(train_model, mode=\"max-autotune\")\n",
    "    \n",
    "    best_model_weights = opt_train(fit_model, train_loader, test_loader, criterion, optimizer, device, epochs, early_stopping_patience)\n",
    "\n",
    "    # Load the best model weights\n",
    "    fit_model.load_state_dict(best_model_weights)\n",
    "\n",
    "    # Define the parameters\n",
    "    mu_fit_init = [np.random.uniform(0.5, 1.5, 1)[0], np.random.uniform(-0.5, 0.5, 1)[0], np.random.uniform(-0.5, 0.5, 1)[0]]\n",
    "\n",
    "    # Create the AddParams2Input layer\n",
    "    add_params_layer = AddParams2Input(mu_fit_init)\n",
    "\n",
    "    # Set all weights in fit model to non-trainable\n",
    "    for param in fit_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Load real data\n",
    "    tree1 = uproot.open(\"LH2Data.root:tree\")\n",
    "    \n",
    "    data1_1 = tree1.arrays([\"mass\", \"pT\", \"xF\", \"phi\", \"costh\", \"weight\"])\n",
    "    \n",
    "    # Number of E906 events\n",
    "    n_E906_events = data1_1.mass.to_numpy().shape[0]\n",
    "    \n",
    "    # Create validation data set\n",
    "    data0_1, data0_2 = train_test_split(events0, test_size = n_E906_events/n_MC_events, shuffle=True)\n",
    "    \n",
    "    X0_val = np.array([(phi, costh) for phi, costh in zip(data0_2[:, 3], data0_2[:, 4])])\n",
    "    \n",
    "    X1_val = np.array([(phi, costh) for phi, costh in zip(data1_1.phi, data1_1.costh)])\n",
    "    \n",
    "    Y0_val = np.zeros(n_E906_events)\n",
    "    Y1_val = np.ones(n_E906_events)\n",
    "    \n",
    "    weight0_val = [(weight_fn(lambda0, mu0, nu0, phi, costh)) for phi, costh in zip(data0_2[:, 5], data0_2[:, 6])]\n",
    "    weight1_val = data1_1.weight.to_numpy()\n",
    "    \n",
    "    X = np.concatenate((X0_val, X1_val))\n",
    "    Y = np.concatenate((Y0_val, Y1_val)).reshape(-1, 1)\n",
    "    weights = np.concatenate((weight0_val, weight1_val)).reshape(-1, 1)\n",
    "    \n",
    "    # Define batch size\n",
    "    batch_size = 1024\n",
    "    \n",
    "    # Create PyTorch datasets and dataloaders\n",
    "    dataset = TensorDataset(torch.Tensor(X), torch.Tensor(Y).float(), torch.Tensor(weights))\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_fn = BMFLoss()\n",
    "    optimizer = torch.optim.Adam(add_params_layer.parameters(), lr=0.001)\n",
    "\n",
    "    # Transfer models to GPU\n",
    "    add_params_layer = add_params_layer.to(device)\n",
    "    fit_model = fit_model.to(device)\n",
    "\n",
    "    # Model summary\n",
    "    print(\"using device : {}\".format(device))\n",
    "    fit_trainable_params = sum(p.numel() for p in fit_model.parameters() if p.requires_grad)\n",
    "    print(fit_model)\n",
    "    print(\"total trainable params in fit model: {}\".format(fit_trainable_params))\n",
    "\n",
    "    total_trainable_params = sum(p.numel() for p in add_params_layer.parameters() if p.requires_grad)\n",
    "    print(add_params_layer)\n",
    "    print(\"total trainable params in fit model: {}\".format(total_trainable_params))\n",
    "\n",
    "    # Fit vals\n",
    "    epochs = 5\n",
    "\n",
    "    losses, fit_vals = fit_fn(epochs, add_params_layer, fit_model, data_loader, device, optimizer, loss_fn)\n",
    "    \n",
    "    \n",
    "    lambda_fit.append(fit_vals[\"lambda\"][-1])\n",
    "    mu_fit.append(fit_vals[\"mu\"][-1])\n",
    "    nu_fit.append(fit_vals[\"nu\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e5cee00-ea7f-4bef-b6a5-1beb5bba6a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda fit = 0.9083 +/- 0.2882\n",
      "mu fit = 0.0309 +/- 0.2403\n",
      "nu fit = -0.0318 +/- 0.1391\n"
     ]
    }
   ],
   "source": [
    "print(\"lambda fit = {:.4f} +/- {:.4f}\".format(np.mean(lambda_fit), np.std(lambda_fit)))\n",
    "print(\"mu fit = {:.4f} +/- {:.4f}\".format(np.mean(mu_fit), np.std(mu_fit)))\n",
    "print(\"nu fit = {:.4f} +/- {:.4f}\".format(np.mean(nu_fit), np.std(nu_fit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe21a9-5c5c-44de-8d64-2dcaaf42e0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
