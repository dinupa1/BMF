{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee288064-dc4d-443a-b6db-81db93af548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import uproot\n",
    "import awkward as ak\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99abfd69-c9d6-4656-8a50-2f6266bcc781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate weights analytically\n",
    "\n",
    "def weight_fn(xx1, xx2, xx3, phi, costh):\n",
    "    weight = 1. + xx1 * costh * costh + 2. * xx2 * costh * np.sqrt(1. - costh * costh) * np.cos(phi) + 0.5 * xx3 * (1. - costh * costh)* np.cos(2. * phi)\n",
    "    return weight / (1. + costh * costh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6949271-c398-4503-8d61-e9debf040050",
   "metadata": {},
   "source": [
    "# Extracting Drell-Yan Angular Coefficients using Neural Network-based Classifiers with E906 LH2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e31528-50d7-40db-8495-ef161882ff12",
   "metadata": {},
   "source": [
    "In this notebook, we use deep neural network-based classifiers to extract the DY angular coefficients. As the first step, let's define our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b721ba00-05e5-4e10-8633-33aae1a79814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMFClassifier(nn.Module):\n",
    "    def __init__(self, input_dim: int = 8, output_dim: int = 1, hidden_dim: int = 64):\n",
    "        super(BMFClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "        self.bn4 = nn.BatchNorm1d(output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.fc1(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(self.fc2(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.bn3(self.fc3(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.bn4(self.fc4(x))\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    # Train step\n",
    "    def train_model(self, criterion, optimizer, device, epochs, early_stopping_patience):\n",
    "\n",
    "        # Load messy MC data\n",
    "        tree = uproot.open(\"BMFData.root:save\")\n",
    "        \n",
    "        data = tree.arrays([\"true_phi\", \"true_costh\", \"mass\", \"pT\", \"xF\", \"phi\", \"costh\"])\n",
    "        \n",
    "        # Number of MC samples\n",
    "        n_MC_events = 10**6\n",
    "        \n",
    "        # Default values for lambda, mu, nu\n",
    "        lambda0, mu0, nu0 = 1.0, 0.0, 0.0\n",
    "        \n",
    "        data0 = data[:n_MC_events]\n",
    "        data1 = data[n_MC_events:2*n_MC_events]\n",
    "        \n",
    "        lambda_vals = np.random.uniform(0.5, 1.5, n_MC_events)\n",
    "        mu_vals = np.random.uniform(-0.5, 0.5, n_MC_events)\n",
    "        nu_vals = np.random.uniform(-0.5, 0.5, n_MC_events)\n",
    "        \n",
    "        X0 = [(mass, pT, xF, phi, costh, theta0, theta1, theta2) for mass, pT, xF, phi, costh, theta0, theta1, theta2 in zip(data0.mass, data0.pT, data0.xF, data0.phi, data0.costh, lambda_vals, mu_vals, nu_vals)]\n",
    "        \n",
    "        X1 = [(mass, pT, xF, phi, costh, theta0, theta1, theta2) for mass, pT, xF, phi, costh, theta0, theta1, theta2 in zip(data1.mass, data1.pT, data1.xF, data1.phi, data1.costh, lambda_vals, mu_vals, nu_vals)]\n",
    "        \n",
    "        Y0 = np.zeros(n_MC_events)\n",
    "        Y1 = np.ones(n_MC_events)\n",
    "        \n",
    "        weight0 = [(weight_fn(lambda0, mu0, nu0, phi, costh)) for phi, costh in zip(data0.true_phi, data0.true_costh)]\n",
    "        weight1 = [(weight_fn(theta0, theta1, theta2, phi, costh)) for theta0, theta1, theta2, phi, costh, in zip(lambda_vals, mu_vals, nu_vals, data1.true_phi, data1.true_costh)]\n",
    "        \n",
    "        X = np.concatenate((X0, X1))\n",
    "        Y = np.concatenate((Y0, Y1)).reshape(-1, 1)\n",
    "        weights = np.concatenate((weight0, weight1)).reshape(-1, 1)\n",
    "        \n",
    "        # Train test split\n",
    "        X_train, X_test, Y_train, Y_test, weights_train, weights_test = train_test_split(X, Y, weights, test_size=0.3, shuffle=True)\n",
    "        \n",
    "        batch_size = 1024\n",
    "        \n",
    "        # Convert data to PyTorch tensors\n",
    "        X_train_tensor = torch.from_numpy(X_train).float()\n",
    "        Y_train_tensor = torch.from_numpy(Y_train).float()\n",
    "        weights_train_tensor = torch.from_numpy(weights_train).float()\n",
    "        \n",
    "        X_test_tensor = torch.from_numpy(X_test).float()\n",
    "        Y_test_tensor = torch.from_numpy(Y_test).float()\n",
    "        weights_test_tensor = torch.from_numpy(weights_test).float()\n",
    "        \n",
    "        # Create PyTorch datasets and dataloaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, Y_train_tensor, weights_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, Y_test_tensor, weights_test_tensor)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        best_model_weights = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Train step\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for batch_inputs, batch_labels, batch_weights in train_loader:\n",
    "                batch_inputs = batch_inputs.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                batch_weights = batch_weights.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_inputs)\n",
    "                loss = criterion(outputs, batch_labels, batch_weights)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * batch_inputs.size(0)\n",
    "                \n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            \n",
    "            # Evaluation\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                running_loss = 0.0\n",
    "                for batch_inputs, batch_labels, batch_weights in test_loader:\n",
    "                    batch_inputs = batch_inputs.to(device)\n",
    "                    batch_labels = batch_labels.to(device)\n",
    "                    batch_weights = batch_weights.to(device)\n",
    "                    \n",
    "                    outputs = self(batch_inputs)\n",
    "                    loss = criterion(outputs, batch_labels, batch_weights)\n",
    "                    \n",
    "                    running_loss += loss.item() * batch_inputs.size(0)\n",
    "                    \n",
    "                validation_loss = running_loss / len(test_loader.dataset)\n",
    "                \n",
    "                # print(\"Epoch {}: Train Loss = {:.4f}, Test Loss = {:.4f}\".format(epoch + 1, epoch_loss, validation_loss))\n",
    "                \n",
    "                # Check for early stopping\n",
    "                if validation_loss < best_loss:\n",
    "                    best_loss = validation_loss\n",
    "                    best_model_weights = self.state_dict()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    print(\"Early stopping at epoch {}\".format(epoch))\n",
    "                    break\n",
    "        return best_model_weights\n",
    "\n",
    "    # Reweight function\n",
    "    def reweight_fn(self, X_val):\n",
    "        # Move the model to CPU for evaluation\n",
    "        model = self.to(torch.device(\"cpu\"))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(torch.Tensor(X_val)).detach().numpy().ravel()\n",
    "            weights = preds / (1.0 - preds)\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    # Fit the model\n",
    "    def fit_fn(self, epochs, add_params_layer, device, optimizer, loss_fn):\n",
    "        # Load messy MC data\n",
    "        tree = uproot.open(\"BMFData.root:save\")\n",
    "        \n",
    "        data = tree.arrays([\"true_phi\", \"true_costh\", \"mass\", \"pT\", \"xF\", \"phi\", \"costh\"])\n",
    "        \n",
    "        # Number of MC samples\n",
    "        n_MC_events = 10**6\n",
    "        \n",
    "        # Default values for lambda, mu, nu\n",
    "        lambda0, mu0, nu0 = 1.0, 0.0, 0.0\n",
    "        \n",
    "        data0_0 = data[:n_MC_events]\n",
    "\n",
    "        data0 = np.array([(mass, pT, xF, phi, costh, true_phi, true_costh) for mass, pT, xF, phi, costh, true_phi, true_costh in zip(data0_0.mass, data0_0.pT, data0_0.xF, data0_0.phi, data0_0.costh, data0_0.true_phi, data0_0.true_costh)])\n",
    "        \n",
    "\n",
    "        # Load real data\n",
    "        tree1 = uproot.open(\"LH2Data.root:tree\")\n",
    "\n",
    "        data1_1 = tree1.arrays([\"mass\", \"pT\", \"xF\", \"phi\", \"costh\", \"weight\"])\n",
    "\n",
    "        # Number of E906 events\n",
    "        n_E906_events = data1_1.mass.to_numpy().shape[0]\n",
    "\n",
    "        # Create validation data set\n",
    "        data0_1, data0_2 = train_test_split(data0, test_size = n_E906_events/n_MC_events, shuffle=True)\n",
    "\n",
    "        X0_val = np.array([(mass, pT, xF, phi, costh) for mass, pT, xF, phi, costh in zip(data0_2[:, 0], data0_2[:, 1], data0_2[:, 2], data0_2[:, 3], data0_2[:, 4])])\n",
    "        \n",
    "        X1_val = np.array([(mass, pT, xF, phi, costh) for mass, pT, xF, phi, costh in zip(data1_1.mass, data1_1.pT, data1_1.xF, data1_1.phi, data1_1.costh)])\n",
    "\n",
    "        Y0_val = np.zeros(n_E906_events)\n",
    "        Y1_val = np.ones(n_E906_events)\n",
    "\n",
    "        weight0_val = [(weight_fn(lambda0, mu0, nu0, phi, costh)) for phi, costh in zip(data0_2[:, 5], data0_2[:, 6])]\n",
    "        weight1_val = data1_1.weight.to_numpy()\n",
    "\n",
    "        X = np.concatenate((X0_val, X1_val))\n",
    "        Y = np.concatenate((Y0_val, Y1_val)).reshape(-1, 1)\n",
    "        weights = np.concatenate((weight0_val, weight1_val)).reshape(-1, 1)\n",
    "\n",
    "        # Define batch size\n",
    "        batch_size = 1024\n",
    "        \n",
    "        # Create PyTorch datasets and dataloaders\n",
    "        dataset = TensorDataset(torch.Tensor(X), torch.Tensor(Y).float(), torch.Tensor(weights))\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        losses = []\n",
    "        fit_vals = {\n",
    "            \"lambda\": [],\n",
    "            \"mu\": [],\n",
    "            \"nu\": []\n",
    "        }\n",
    "\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            add_params_layer.train()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for batch_inputs, batch_labels, batch_weights in data_loader:\n",
    "                batch_inputs = batch_inputs.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                batch_weights = batch_weights.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                param_input = add_params_layer(batch_inputs)\n",
    "                output = self(param_input)\n",
    "                \n",
    "                # Compute the loss\n",
    "                loss = loss_fn(output, batch_labels, batch_weights)\n",
    "                \n",
    "                # Backward pass and update weights\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * batch_inputs.size(0)\n",
    "                \n",
    "            epoch_loss = running_loss / len(data_loader.dataset)\n",
    "            # print(\"epoch : {}, loss = {:.4f}, lambda = {:.4f}, mu = {:.4f}, nu = {:.4f}\".format(epoch + 1, epoch_loss,\n",
    "            #                                                                                 add_params_layer.params[0].item(),\n",
    "            #                                                                                 add_params_layer.params[1].item(),\n",
    "            #                                                                                 add_params_layer.params[2].item()))\n",
    "            losses.append(epoch_loss)\n",
    "            fit_vals[\"lambda\"].append(add_params_layer.params[0].item())\n",
    "            fit_vals[\"mu\"].append(add_params_layer.params[1].item())\n",
    "            fit_vals[\"nu\"].append(add_params_layer.params[2].item())\n",
    "            \n",
    "        return fit_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99353960-d6bc-4fb0-b036-f4049b77c0bd",
   "metadata": {},
   "source": [
    "We extract the angular coefficients in two steps:\n",
    "\n",
    "## Step 1: Parameterize the neural network with angular coefficients\n",
    "\n",
    "In this step, we parameterize the neural network with $\\lambda$, $\\mu$, $\\nu$ values. This is done during the training step. The input features to the neural network are `mass`, `pT`, `xF`, `phi`, `costh`, `lambda`, `mu`, and `nu`.\n",
    "\n",
    "## Step 2: Extract the parameters using the gradient descent algorithm\n",
    "\n",
    "Since we have parameterized the neural network in step 1, we can fix the trained weights in the neural network and extract the angular coefficients by minimizing the loss with the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99382a70-362d-4781-a9fd-c3f46681f79b",
   "metadata": {},
   "source": [
    "Let's select the `cuda` device for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd90e9f9-c635-409b-9202-aad1c517048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61dacf-5770-47f9-81f9-6ea00f0b5f73",
   "metadata": {},
   "source": [
    "Let's define a custom loss function to classify the classes with weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d814c0-c65b-4714-8578-b2b9fcc3e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "class BMFLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BMFLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, outputs, targets, weights):\n",
    "        weighted_targets = targets * weights + (1 - targets) * (1 - weights)\n",
    "        criterion = nn.BCELoss()\n",
    "        loss = criterion(outputs, weighted_targets)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087a9080-7d10-4973-9b1a-abab6ff1c53e",
   "metadata": {},
   "source": [
    "Let's define a parameter extraction layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c664cd-0014-4b00-9c2e-148920cd1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module used to add parameter for fitting\n",
    "class AddParams2Input(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(AddParams2Input, self).__init__()\n",
    "        self.params = nn.Parameter(torch.Tensor(params), requires_grad=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_params = torch.ones((inputs.size(0), 1), device=inputs.device) * self.params.to(device=inputs.device)\n",
    "        concatenated = torch.cat([inputs, batch_params], dim=-1)\n",
    "        return concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d94bd03-73a7-4f9a-ba6d-35e172b36d15",
   "metadata": {},
   "source": [
    "Let's define our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1afd161-3f43-48fd-b800-f1f0a44c4b49",
   "metadata": {},
   "source": [
    "We use the `Adam` optimizer for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d4208-7990-44ec-9f52-cd4a3335aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of epochs\n",
    "epochs = 200\n",
    "\n",
    "# Define early stopping patience\n",
    "early_stopping_patience = 20\n",
    "\n",
    "# Define number of iterations\n",
    "iterations = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032c8ec-1781-4004-993d-436c224ad814",
   "metadata": {},
   "source": [
    "## Fit Model to Extract the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb353501-e1fd-4109-9966-6b2ee8a12303",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_fit, mu_fit, nu_fit = [], [], []\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    print(\"iteration = {}\".format(i+1))\n",
    "\n",
    "    fit_model = BMFClassifier(input_dim=8, hidden_dim=64)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = BMFLoss()\n",
    "    optimizer = optim.Adam(fit_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Move the model to GPU if available\n",
    "    fit_model = fit_model.to(device=device)\n",
    "\n",
    "    # Compile the model\n",
    "    opt_model = torch.compile(fit_model)\n",
    "\n",
    "    # Model summary\n",
    "    # print(\"using device : {}\".format(device))\n",
    "    # total_trainable_params = sum(p.numel() for p in fit_model.parameters() if p.requires_grad)\n",
    "    # print(fit_model)\n",
    "    # print('total trainable params: {}'.format(total_trainable_params))\n",
    "\n",
    "    # Train the model\n",
    "    best_model_weights = opt_model.train_model(criterion, optimizer, device, epochs, early_stopping_patience)\n",
    "\n",
    "    # Load the best model weights\n",
    "    fit_model.load_state_dict(best_model_weights)\n",
    "\n",
    "    # Initialize the parameters\n",
    "    fit_init = [np.random.uniform(0.5, 1.5, 1)[0], np.random.uniform(-0.5, 0.5, 1)[0], np.random.uniform(-0.5, 0.5, 1)[0]]\n",
    "\n",
    "    # Create the AddParams2Input layer\n",
    "    add_params_layer = AddParams2Input(fit_init)\n",
    "\n",
    "    # Set all weights in fit model to non-trainable\n",
    "    for param in fit_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_fn = BMFLoss()\n",
    "    optimizer = torch.optim.Adam(add_params_layer.parameters(), lr=0.001)\n",
    "\n",
    "    # Transfer models to GPU\n",
    "    add_params_layer = add_params_layer.to(device)\n",
    "    fit_model = fit_model.to(device)\n",
    "\n",
    "    # Model summary\n",
    "    # print(\"using device : {}\".format(device))\n",
    "    # fit_trainable_params = sum(p.numel() for p in fit_model.parameters() if p.requires_grad)\n",
    "    # print(fit_model)\n",
    "    # print(\"total trainable params in fit model: {}\".format(fit_trainable_params))\n",
    "\n",
    "    # total_trainable_params = sum(p.numel() for p in add_params_layer.parameters() if p.requires_grad)\n",
    "    # print(add_params_layer)\n",
    "    # print(\"total trainable params in fit model: {}\".format(total_trainable_params))\n",
    "\n",
    "    fit_vals = fit_model.fit_fn(epochs, add_params_layer, device, optimizer, loss_fn)\n",
    "    \n",
    "    lambda_fit.append(fit_vals[\"lambda\"][-1])\n",
    "    mu_fit.append(fit_vals[\"mu\"][-1])\n",
    "    nu_fit.append(fit_vals[\"nu\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989282a7-a738-4567-9cb5-75420f9aa0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lambda fit = {:.4f} +/- {:.4f}\".format(np.mean(lambda_fit), np.std(lambda_fit)))\n",
    "print(\"mu fit = {:.4f} +/- {:.4f}\".format(np.mean(mu_fit), np.std(mu_fit)))\n",
    "print(\"nu fit = {:.4f} +/- {:.4f}\".format(np.mean(nu_fit), np.std(nu_fit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d1bc9-2c2c-4998-aac3-378cfbf822ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
