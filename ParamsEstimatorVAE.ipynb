{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ffc07b-f66f-4ceb-b0d5-4fb463e0ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import uproot\n",
    "import awkward as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd6047d-918f-4654-b490-c32c28da79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b55b72c-e1f9-4c9a-a7ab-9f291c0af72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate weights analytically\n",
    "\n",
    "def weight_fn(thetas, angles):\n",
    "    theta0, theta1, theta2 = thetas[:, 0], thetas[:, 1], thetas[:, 2]\n",
    "    phi, costh = angles[:, 0], angles[:, 1]\n",
    "    weight = 1. + theta0* costh * costh + 2.* theta1* costh * np.sqrt(1. - costh * costh) * np.cos(phi) + 0.5* theta2* (1. - costh * costh)* np.cos(2. * phi)\n",
    "    return weight / (1. + costh * costh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92ebe88-73e0-45e2-9b3c-7fbf6aaf56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load E906 simulated data\n",
    "\n",
    "nevents = 10**6\n",
    "\n",
    "tree = uproot.open(\"BMFData.root:save\")\n",
    "events = tree.arrays([\"mass\", \"pT\", \"xF\", \"phi\", \"costh\", \"true_phi\", \"true_costh\"])\n",
    "\n",
    "X = np.array([(mass, pT, xF, phi, costh) for mass, pT, xF, phi, costh in zip(events.mass, events.pT, events.xF, events.phi, events.costh)])\n",
    "Y = np.array([(true_phi, true_costh) for true_phi, true_costh in zip(events.true_phi, events.true_costh)])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=nevents, shuffle=True)\n",
    "thetas = np.random.uniform(-1., 1, (nevents, 3))\n",
    "W_train = weight_fn(thetas, Y_train).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30fc3a3-bf98-4b0c-b161-e286ab003b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "num_epochs = 100\n",
    "\n",
    "# Convert to torch tensor\n",
    "X_tensor = torch.from_numpy(X_train).float()\n",
    "weight_tensor = torch.from_numpy(W_train).float()\n",
    "thetas_tensor = torch.from_numpy(thetas).float()\n",
    "\n",
    "dataset = TensorDataset(X_tensor, weight_tensor, thetas_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78101472-8b4f-456e-8f09-32f20f3651bb",
   "metadata": {},
   "source": [
    "# Variational Autoencoders for Parameter Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d8dd872-5191-4b78-a9c7-c7dd7ee06dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamsEstimator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, params_dim):\n",
    "        super(ParamsEstimator, self).__init__()\n",
    "\n",
    "        self.fc_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim, latent_dim, bias=True),\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim, bias=True)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim, bias=True)\n",
    "\n",
    "        self.fc_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, params_dim, bias=True),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.fc_encoder(x)\n",
    "        mu, logvar = self.fc_mu(h), self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_decoder(z)\n",
    "        return h\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14eaba2a-e91d-4b78-b4cc-e568e7f4b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimatorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EstimatorLoss, self).__init__()\n",
    "\n",
    "    def forward(self, reco_params, weight, params, mu, logvar):\n",
    "        reco_loss = (weight* (reco_params - params)*(reco_params - params)).sum()\n",
    "        kld_loss = torch.sum(1. + logvar - mu.pow(2) - logvar.exp())\n",
    "        return (reco_loss + kld_loss)/params.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af5e541b-0b68-4aa5-a0ee-b2ce79399306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VAE\n",
    "input_dim = 5\n",
    "hidden_dim = 64\n",
    "latent_dim = 32\n",
    "params_dim = 3\n",
    "estimator = ParamsEstimator(input_dim, hidden_dim, latent_dim, params_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d23692-bbcb-493c-bc15-b4418ab150c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = optim.Adam(estimator.parameters(), lr=0.001)\n",
    "loss_fn = EstimatorLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32967a04-aeda-41f8-a2d0-3d6bea6b9f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch [0/100], Loss: -1.3560248406020016e+22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     reco_params, mu, logvar \u001b[38;5;241m=\u001b[39m estimator(batch_x)\n\u001b[1;32m      7\u001b[0m     loss_batch \u001b[38;5;241m=\u001b[39m loss_fn(reco_params, batch_weight, batch_params, mu, logvar)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mloss_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/BMFNet/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/BMFNet/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    estimator.train()\n",
    "    for batch_x, batch_weight, batch_params in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        reco_params, mu, logvar = estimator(batch_x)\n",
    "        loss_batch = loss_fn(reco_params, batch_weight, batch_params, mu, logvar)\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"===> Epoch [{epoch}/{num_epochs}], Loss: {loss_batch.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335ddd8-fb98-469c-94df-5eb52732a3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
